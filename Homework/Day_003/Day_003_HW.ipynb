{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業目標]\n",
    "持續接觸有關機器學習的相關專案與最新技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "透過觀察頂尖公司的機器學習文章，來了解各公司是怎麼應用機器學習在實際的專案上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業]\n",
    "今天的作業希望大家能夠看看全球機器學習巨頭們在做的機器學習專案。以 google 為例，下圖是 Google 內部專案使用機器學習的數量，隨著時間進展，現在早已超過 2000 個專案在使用機器學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/800/1*U_L8qI8RmYS-MOBrYvXhSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下幫同學整理幾間知名企業的 blog 或機器學習網站 (自行搜尋也可)，這些網站都會整理最新的機器學習專案或者是技術文章，請挑選一篇文章閱讀並試著回答\n",
    "1. 專案的目標？ (要解決什麼問題）\n",
    "2. 使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\n",
    "3. 資料來源？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Google AI blog](https://ai.googleblog.com/)\n",
    "- [Facebook Research blog](https://research.fb.com/blog/)\n",
    "- [Apple machine learning journal](https://machinelearning.apple.com/)\n",
    "- [機器之心](https://www.jiqizhixin.com/)\n",
    "- [雷鋒網](http://www.leiphone.com/category/ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DAY3作業]\n",
    "\n",
    "參考文章網址:https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html\n",
    "\n",
    "參考文章:ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations\n",
    "\n",
    "\n",
    "    一、專案的目標？\n",
    "    \n",
    "        -增進NLP對於較大Dataset的意義的準確性\n",
    "        \n",
    "    二、使用的技術是？\n",
    "    \n",
    "        -更有效地分配模型的容量:減少80%的參數設置，但只以很小的代價達到(<1%)\n",
    "          。factorization of the embedding parametrization — the embedding matrix is split \n",
    "            between input-level embeddings with a relatively-low dimension (e.g., 128), while the \n",
    "            hidden-layer embeddings use higher dimensionalities (768 as in the BERT case, or \n",
    "            more). \n",
    "        -Parameter sharing achieves a 90% parameter reduction for the attention-feedforward block\n",
    "          。 Transformer-based neural network architectures (such as BERT, XLNet, and RoBERTa) \n",
    "             rely on independent layers stacked on top of each other\n",
    "             \n",
    "    三、資料來源？\n",
    "    \n",
    "        -訓練資料的來源主要來自:\n",
    "            。已有的文本\n",
    "            。維基百科\n",
    "        -評估模型的語言理解能力\n",
    "            。 RACE dataset (e.g., similar to the SAT Reading Test)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
